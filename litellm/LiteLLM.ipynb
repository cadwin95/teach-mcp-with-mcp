{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.4999999999999995e-05\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion \n",
    "\n",
    "response = completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hey, how's it going?\"}],\n",
    "            mock_response=\"Hello world\",\n",
    "        )\n",
    "\n",
    "print(response._hidden_params[\"response_cost\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81394, 3029, 4435, 11, 420, 374, 856, 1988, 925, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from litellm import encode, decode\n",
    "\n",
    "sample_text = \"Hellö World, this is my input string!\"\n",
    "# openai encoding + decoding\n",
    "openai_tokens = encode(model=\"gpt-4.1-nano\", text=sample_text)\n",
    "print(openai_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellö World, this is my input string!\n"
     ]
    }
   ],
   "source": [
    "# Decoding is supported for anthropic, cohere, llama2 and openai.\n",
    "from litellm import encode, decode\n",
    "\n",
    "sample_text = \"Hellö World, this is my input string!\"\n",
    "# openai encoding + decoding\n",
    "openai_tokens = encode(model=\"gpt-4.1-nano\", text=sample_text)\n",
    "openai_text = decode(model=\"gpt-4.1-nano\", tokens=openai_tokens)\n",
    "print(openai_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "from litellm import token_counter\n",
    "\n",
    "messages = [{\"user\": \"role\", \"content\": \"Hey, how's it going\"}]\n",
    "print(token_counter(model=\"gpt-3.5-turbo\", messages=messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5e-06 1.9999999999999998e-05\n"
     ]
    }
   ],
   "source": [
    "from litellm import cost_per_token\n",
    "\n",
    "prompt_tokens =  5\n",
    "completion_tokens = 10\n",
    "prompt_tokens_cost_usd_dollar, completion_tokens_cost_usd_dollar = cost_per_token(model=\"gpt-3.5-turbo\", prompt_tokens=prompt_tokens, completion_tokens=completion_tokens)\n",
    "\n",
    "print(prompt_tokens_cost_usd_dollar, completion_tokens_cost_usd_dollar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "litellm.APIConnectionError: No module named 'boto3'\nTraceback (most recent call last):\n  File \"c:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\main.py\", line 2756, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<12 lines>...\n        api_base=api_base,\n    )\n  File \"c:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\llms\\bedrock\\chat\\converse_handler.py\", line 304, in completion\n    credentials: Credentials = self.get_credentials(\n                               ~~~~~~~~~~~~~~~~~~~~^\n        aws_access_key_id=aws_access_key_id,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<7 lines>...\n        aws_sts_endpoint=aws_sts_endpoint,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"c:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\llms\\bedrock\\base_aws_llm.py\", line 211, in get_credentials\n    credentials, _cache_ttl = self._auth_with_env_vars()\n                              ~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"c:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\llms\\bedrock\\base_aws_llm.py\", line 509, in _auth_with_env_vars\n    import boto3\nModuleNotFoundError: No module named 'boto3'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\main.py:2756\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   2755\u001b[39m     model = model.replace(\u001b[33m\"\u001b[39m\u001b[33mconverse/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2756\u001b[39m     response = \u001b[43mbedrock_converse_chat_completion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2761\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   2763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2766\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2767\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2768\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2769\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2772\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m bedrock_route == \u001b[33m\"\u001b[39m\u001b[33mconverse_like\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\llms\\bedrock\\chat\\converse_handler.py:304\u001b[39m, in \u001b[36mBedrockConverseLLM.completion\u001b[39m\u001b[34m(self, model, messages, api_base, custom_prompt_dict, model_response, encoding, logging_obj, optional_params, acompletion, timeout, litellm_params, logger_fn, extra_headers, client)\u001b[39m\n\u001b[32m    300\u001b[39m litellm_params[\n\u001b[32m    301\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maws_region_name\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    302\u001b[39m ] = aws_region_name  \u001b[38;5;66;03m# [DO NOT DELETE] important for async calls\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m credentials: Credentials = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_credentials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43maws_access_key_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43maws_access_key_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43maws_secret_access_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43maws_secret_access_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43maws_session_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43maws_session_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43maws_region_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43maws_region_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43maws_session_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43maws_session_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43maws_profile_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43maws_profile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43maws_role_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43maws_role_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43maws_web_identity_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43maws_web_identity_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43maws_sts_endpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43maws_sts_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m### SET RUNTIME ENDPOINT ###\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\llms\\bedrock\\base_aws_llm.py:211\u001b[39m, in \u001b[36mBaseAWSLLM.get_credentials\u001b[39m\u001b[34m(self, aws_access_key_id, aws_secret_access_key, aws_session_token, aws_region_name, aws_session_name, aws_profile_name, aws_role_name, aws_web_identity_token, aws_sts_endpoint)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     credentials, _cache_ttl = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auth_with_env_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28mself\u001b[39m.iam_cache.set_cache(cache_key, credentials, ttl=_cache_ttl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\llms\\bedrock\\base_aws_llm.py:509\u001b[39m, in \u001b[36mBaseAWSLLM._auth_with_env_vars\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[33;03mAuthenticate with AWS Environment Variables\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m509\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mboto3\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tracer.trace(\u001b[33m\"\u001b[39m\u001b[33mboto3.Session()\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'boto3'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m messages = [{\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHey, how\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms it going\u001b[39m\u001b[33m\"\u001b[39m}]\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlitellm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m completion, completion_cost\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbedrock/anthropic.claude-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# pass your response from completion to completion_cost\u001b[39;00m\n\u001b[32m     11\u001b[39m cost = completion_cost(completion_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\utils.py:1283\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1280\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1281\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1282\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1283\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\utils.py:1161\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1159\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1160\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1162\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\main.py:3241\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3240\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3244\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3247\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2239\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2238\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2239\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2241\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2215\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2208\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(\n\u001b[32m   2209\u001b[39m                 message=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(exception_provider, error_str),\n\u001b[32m   2210\u001b[39m                 llm_provider=custom_llm_provider,\n\u001b[32m   2211\u001b[39m                 model=model,\n\u001b[32m   2212\u001b[39m                 request=original_exception.request,\n\u001b[32m   2213\u001b[39m             )\n\u001b[32m   2214\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2215\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(\n\u001b[32m   2216\u001b[39m                 message=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2217\u001b[39m                     \u001b[38;5;28mstr\u001b[39m(original_exception), traceback.format_exc()\n\u001b[32m   2218\u001b[39m                 ),\n\u001b[32m   2219\u001b[39m                 llm_provider=custom_llm_provider,\n\u001b[32m   2220\u001b[39m                 model=model,\n\u001b[32m   2221\u001b[39m                 request=httpx.Request(\n\u001b[32m   2222\u001b[39m                     method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m, url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.openai.com/v1/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2223\u001b[39m                 ),  \u001b[38;5;66;03m# stub the request\u001b[39;00m\n\u001b[32m   2224\u001b[39m             )\n\u001b[32m   2225\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2226\u001b[39m     \u001b[38;5;66;03m# LOGGING\u001b[39;00m\n\u001b[32m   2227\u001b[39m     exception_logging(\n\u001b[32m   2228\u001b[39m         logger_fn=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2229\u001b[39m         additional_args={\n\u001b[32m   (...)\u001b[39m\u001b[32m   2233\u001b[39m         exception=e,\n\u001b[32m   2234\u001b[39m     )\n",
      "\u001b[31mAPIConnectionError\u001b[39m: litellm.APIConnectionError: No module named 'boto3'\nTraceback (most recent call last):\n  File \"c:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\main.py\", line 2756, in completion\n    response = bedrock_converse_chat_completion.completion(\n        model=model,\n    ...<12 lines>...\n        api_base=api_base,\n    )\n  File \"c:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\llms\\bedrock\\chat\\converse_handler.py\", line 304, in completion\n    credentials: Credentials = self.get_credentials(\n                               ~~~~~~~~~~~~~~~~~~~~^\n        aws_access_key_id=aws_access_key_id,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<7 lines>...\n        aws_sts_endpoint=aws_sts_endpoint,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"c:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\llms\\bedrock\\base_aws_llm.py\", line 211, in get_credentials\n    credentials, _cache_ttl = self._auth_with_env_vars()\n                              ~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"c:\\Users\\infomax\\Documents\\teach-mcp-with-mcp\\.venv\\Lib\\site-packages\\litellm\\llms\\bedrock\\base_aws_llm.py\", line 509, in _auth_with_env_vars\n    import boto3\nModuleNotFoundError: No module named 'boto3'\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"user\": \"role\", \"content\": \"Hey, how's it going\"}]\n",
    "\n",
    "from litellm import completion, completion_cost\n",
    "\n",
    "response = completion(\n",
    "            model=\"bedrock/anthropic.claude-v2\",\n",
    "            messages=messages,\n",
    "            request_timeout=200,\n",
    "        )\n",
    "# pass your response from completion to completion_cost\n",
    "cost = completion_cost(completion_response=response)\n",
    "formatted_string = f\"${float(cost):.10f}\"\n",
    "print(formatted_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
